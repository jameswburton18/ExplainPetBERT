{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/ExplainPetBERT/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from src.helper_functions import Config, prepare_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating with:\n",
      "{'config': 'all_text', 'fast_dev_run': False, 'batch_size': 32, 'ds_name': 'james-burton/vet_month_1_all_text', 'version': 'all_text'}\n",
      "\n",
      "\n",
      "{'config': 'all_text', 'fast_dev_run': False, 'do_train': True, 'do_predict': True, 'tags': ['bert', '1 month'], 'batch_size': 32, 'model_base': 'bert-base-uncased', 'output_root': 'models/', 'num_epochs': 50, 'early_stopping_patience': 3, 'grad_accumulation_steps': 1, 'seed': 42, 'logging_steps': 10, 'lr_scheduler': 'linear', 'warmup_ratio': 0, 'device': 'cuda', 'num_workers': 1, 'resume_from_checkpoint': False, 'predict_batch_size': 16, 'save_total_limit': 1, 'pytorch2.0': True, 'max_length': 512, 'ds_name': 'james-burton/vet_month_1_all_text', 'version': 'all_text'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.95k/1.95k [00:00<00:00, 3.46MB/s]\n",
      "Downloading data: 100%|██████████| 739k/739k [00:00<00:00, 2.45MB/s]\n",
      "Downloading data: 100%|██████████| 2.42M/2.42M [00:00<00:00, 14.6MB/s]\n",
      "Downloading data: 100%|██████████| 433k/433k [00:00<00:00, 2.87MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  4.71it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3245.53it/s]\n",
      "Generating test split: 100%|██████████| 2184/2184 [00:00<00:00, 274681.70 examples/s]\n",
      "Generating train split: 100%|██████████| 7206/7206 [00:00<00:00, 487274.16 examples/s]\n",
      "Generating validation split: 100%|██████████| 1272/1272 [00:00<00:00, 323029.47 examples/s]\n",
      "Map: 100%|██████████| 2184/2184 [00:00<00:00, 3351.31 examples/s]\n",
      "Map: 100%|██████████| 7206/7206 [00:02<00:00, 3284.74 examples/s]\n",
      "Map: 100%|██████████| 1272/1272 [00:00<00:00, 3386.87 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map:   0%|          | 0/2184 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 2184/2184 [00:01<00:00, 1310.41 examples/s]\n",
      "Map: 100%|██████████| 7206/7206 [00:05<00:00, 1263.87 examples/s]\n",
      "Map: 100%|██████████| 1272/1272 [00:00<00:00, 1312.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Import yaml file\n",
    "with open(\"../configs/train_default.yaml\") as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "config_type = \"all_text\"\n",
    "# Update default args with chosen config\n",
    "if config_type != \"default\":\n",
    "    with open(\"../configs/train_configs.yaml\") as f:\n",
    "        yaml_configs = yaml.safe_load_all(f)\n",
    "        yaml_args = next(conf for conf in yaml_configs if conf[\"config\"] == config_type)\n",
    "    args.update(yaml_args)\n",
    "    print(f\"Updating with:\\n{yaml_args}\\n\")\n",
    "print(f\"\\n{args}\\n\")\n",
    "\n",
    "# Dataset\n",
    "di = Config(\"../configs/dataset_info.yaml\")\n",
    "dataset = load_dataset(\n",
    "    args[\"ds_name\"],\n",
    "    download_mode=\"force_redownload\",\n",
    ")\n",
    "dataset = prepare_text(\n",
    "    dataset=dataset,\n",
    "    di=di,\n",
    "    version=args[\"version\"],\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args[\"model_base\"],\n",
    "    num_labels=2,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_base\"])\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def encode(examples):\n",
    "    return {\n",
    "        \"label\": np.array([examples[\"labels\"]]),\n",
    "        **tokenizer(\n",
    "            examples[\"text\"],\n",
    "            # truncation=True,\n",
    "            # padding=\"max_length\",\n",
    "            # max_length=args[\"max_length\"],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode)  # , load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055925617540938104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(ids) > 512 for ids in dataset[\"train\"][\"input_ids\"]]) / len(\n",
    "    dataset[\"train\"][\"input_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
