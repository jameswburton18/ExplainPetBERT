{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/CodingProjects/ExplainPetBERT/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_from_disk, load_dataset\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from src.utils import ConfigLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating with:\n",
      "{'config': 'all_text', 'fast_dev_run': False, 'batch_size': 32, 'ds_name': 'james-burton/vet_month_1_all_text', 'version': 'all_text'}\n",
      "\n",
      "\n",
      "{'config': 'all_text', 'fast_dev_run': False, 'do_train': True, 'do_predict': True, 'tags': ['bert', '1 month'], 'batch_size': 32, 'model_base': 'bert-base-uncased', 'output_root': 'models/', 'num_epochs': 50, 'early_stopping_patience': 3, 'grad_accumulation_steps': 1, 'seed': 42, 'logging_steps': 10, 'lr_scheduler': 'linear', 'warmup_ratio': 0, 'device': 'cuda', 'num_workers': 1, 'resume_from_checkpoint': False, 'predict_batch_size': 16, 'save_total_limit': 1, 'pytorch2.0': True, 'max_length': 512, 'ds_name': 'james-burton/vet_month_1_all_text', 'version': 'all_text'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.95k/1.95k [00:00<00:00, 3.46MB/s]\n",
      "Downloading data: 100%|██████████| 739k/739k [00:00<00:00, 2.45MB/s]\n",
      "Downloading data: 100%|██████████| 2.42M/2.42M [00:00<00:00, 14.6MB/s]\n",
      "Downloading data: 100%|██████████| 433k/433k [00:00<00:00, 2.87MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  4.71it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3245.53it/s]\n",
      "Generating test split: 100%|██████████| 2184/2184 [00:00<00:00, 274681.70 examples/s]\n",
      "Generating train split: 100%|██████████| 7206/7206 [00:00<00:00, 487274.16 examples/s]\n",
      "Generating validation split: 100%|██████████| 1272/1272 [00:00<00:00, 323029.47 examples/s]\n",
      "Map: 100%|██████████| 2184/2184 [00:00<00:00, 3351.31 examples/s]\n",
      "Map: 100%|██████████| 7206/7206 [00:02<00:00, 3284.74 examples/s]\n",
      "Map: 100%|██████████| 1272/1272 [00:00<00:00, 3386.87 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map:   0%|          | 0/2184 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (610 > 512). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 2184/2184 [00:01<00:00, 1310.41 examples/s]\n",
      "Map: 100%|██████████| 7206/7206 [00:05<00:00, 1263.87 examples/s]\n",
      "Map: 100%|██████████| 1272/1272 [00:00<00:00, 1312.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Import yaml file\n",
    "with open(\"../configs/train_default.yaml\") as f:\n",
    "    args = yaml.safe_load(f)\n",
    "\n",
    "config_type = \"all_text\"\n",
    "# Update default args with chosen config\n",
    "if config_type != \"default\":\n",
    "    with open(\"../configs/train_configs.yaml\") as f:\n",
    "        yaml_configs = yaml.safe_load_all(f)\n",
    "        yaml_args = next(conf for conf in yaml_configs if conf[\"config\"] == config_type)\n",
    "    args.update(yaml_args)\n",
    "    print(f\"Updating with:\\n{yaml_args}\\n\")\n",
    "print(f\"\\n{args}\\n\")\n",
    "\n",
    "# Dataset\n",
    "di = Config(\"../configs/dataset_info.yaml\")\n",
    "dataset = load_dataset(\n",
    "    args[\"ds_name\"],\n",
    "    download_mode=\"force_redownload\",\n",
    ")\n",
    "dataset = prepare_text(\n",
    "    dataset=dataset,\n",
    "    di=di,\n",
    "    version=args[\"version\"],\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args[\"model_base\"],\n",
    "    num_labels=2,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(args[\"model_base\"])\n",
    "\n",
    "\n",
    "# Tokenize the dataset\n",
    "def encode(examples):\n",
    "    return {\n",
    "        \"label\": np.array([examples[\"labels\"]]),\n",
    "        **tokenizer(\n",
    "            examples[\"text\"],\n",
    "            # truncation=True,\n",
    "            # padding=\"max_length\",\n",
    "            # max_length=args[\"max_length\"],\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "dataset = dataset.map(encode)  # , load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055925617540938104"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(ids) > 512 for ids in dataset[\"train\"][\"input_ids\"]]) / len(\n",
    "    dataset[\"train\"][\"input_ids\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating with:\n",
      "{'config': 'vet_10b_baseline', 'my_text_model': 'james-burton/vet_50b', 'ds_name': 'james-burton/vet_month_1b_all_text', 'text_model_base': 'bert-base-uncased', 'model_type': 'all_text', 'ord_ds_name': 'james-burton/vet_month_1b_ordinal'}\n",
      "\n",
      "\n",
      "{'categorical_cols': ['gender', 'neutered', 'species', 'insured'], 'numerical_cols_long': ['age_at_consult', 'Diseases of the ear or mastoid process', 'Mental, behavioural or neurodevelopmental disorders', 'Diseases of the blood or blood-forming organs', 'Diseases of the circulatory system', 'Dental', 'Developmental anomalies', 'Diseases of the digestive system', 'Endocrine, nutritional or metabolic diseases', 'Diseases of the Immune system', 'Certain infectious or parasitic diseases', 'Diseases of the skin', 'Diseases of the musculoskeletal system or connective tissue', 'Neoplasms', 'Diseases of the nervous system', 'Diseases of the visual system', 'Certain conditions originating in the perinatal period', 'Pregnancy, childbirth or the puerperium', 'Diseases of the respiratory system', 'Injury, poisoning or certain other consequences of external causes', 'Diseases of the genitourinary system'], 'numerical_cols': ['age_at_consult', 'Ear_or_Mastoid', 'Mental_Behavioral_or_Neuro', 'Blood_or_Blood-forming', 'Circulatory', 'Dental', 'Developmental', 'Digestive', 'Endocrine_Nutritional_or_Metabolic', 'Immune', 'Infectious_or_Parasitic', 'Skin', 'Musculoskeletal_or_Connective_Tissue', 'Neoplasms', 'Nervous', 'Visual', 'Perinatal', 'Pregnancy_Childbirth_or_Puerperium', 'Respiratory', 'Injury_Poisoning_or_External_Causes', 'Genitourinary'], 'text_cols': ['practice_id', 'premise_id', 'breed', 'region', 'record'], 'label_col': 'labels', 'config': 'vet_10b_baseline', 'my_text_model': 'james-burton/vet_50b', 'ds_name': 'james-burton/vet_month_1b_all_text', 'text_model_base': 'bert-base-uncased', 'model_type': 'all_text', 'ord_ds_name': 'james-burton/vet_month_1b_ordinal'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 1.94k/1.94k [00:00<00:00, 16.5MB/s]\n",
      "Downloading data: 100%|██████████| 754k/754k [00:00<00:00, 4.36MB/s]\n",
      "Downloading data: 100%|██████████| 2.44M/2.44M [00:00<00:00, 16.8MB/s]\n",
      "Downloading data: 100%|██████████| 453k/453k [00:00<00:00, 3.54MB/s]]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00,  6.47it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 3086.32it/s]\n",
      "Generating test split: 100%|██████████| 2184/2184 [00:00<00:00, 337224.27 examples/s]\n",
      "Generating train split: 100%|██████████| 7206/7206 [00:00<00:00, 473443.42 examples/s]\n",
      "Generating validation split: 100%|██████████| 1272/1272 [00:00<00:00, 279181.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "config_type = \"vet_10b_baseline\"\n",
    "\n",
    "di = ConfigLoader(\n",
    "    config_type, \"../configs/shap_configs.yaml\", \"../configs/dataset_default.yaml\"\n",
    ")\n",
    "# Data\n",
    "test_df = load_dataset(\n",
    "    di.ds_name, split=\"test\", download_mode=\"force_redownload\"\n",
    ").to_pandas()\n",
    "test_df = test_df.sample(1000, random_state=55)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    di.text_model_base, model_max_length=512, truncation=True\n",
    ")\n",
    "\n",
    "\n",
    "# Define how to convert all columns to a single string\n",
    "def cols_to_str_fn(array):\n",
    "    return \" | \".join(\n",
    "        [\n",
    "            f\"{col}: {val}\"\n",
    "            for col, val in zip(\n",
    "                di.categorical_cols + di.numerical_cols + di.text_cols, array\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "x = list(\n",
    "    map(\n",
    "        cols_to_str_fn,\n",
    "        test_df[di.categorical_cols + di.numerical_cols + di.text_cols].values,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def encode(examples):\n",
    "    return {\n",
    "        \"label\": np.array([examples[\"labels\"]]),\n",
    "        **tokenizer(\n",
    "            examples[\"text\"],\n",
    "            # truncation=True,\n",
    "            # padding=\"max_length\",\n",
    "            # max_length=args[\"max_length\"],\n",
    "        ),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39msum\u001b[39m([\u001b[39mlen\u001b[39m(ids) \u001b[39m>\u001b[39m \u001b[39m512\u001b[39m \u001b[39mfor\u001b[39;00m ids \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]]) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(\n\u001b[1;32m      2\u001b[0m     x[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "sum([len(ids) > 512 for ids in x[\"input_ids\"]]) / len(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [len(tokenizer.tokenize(sentence)) for sentence in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([sent > 512 for sent in x1]) / len(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
