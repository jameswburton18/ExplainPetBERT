{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "import lightgbm as lgb\n",
    "from src.models import WeightedEnsemble, StackModel, AllAsTextModel\n",
    "from src.utils import ConfigLoader, compute_metrics\n",
    "import xgboost as xgb\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from src.utils import format_text_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(config_type):\n",
    "    # Shap args\n",
    "    args = ConfigLoader(\n",
    "        config_type, \"../configs/shap_configs.yaml\", \"../configs/dataset_default.yaml\"\n",
    "    )\n",
    "    # Data\n",
    "    all_text_versions = [\n",
    "        \"all_text\",\n",
    "        \"all_as_text\",\n",
    "        \"all_as_text_base_reorder\",\n",
    "        \"all_as_text_tnt_reorder\",\n",
    "    ]\n",
    "    ds_name = args.ds_name\n",
    "    train_df = load_dataset(\n",
    "        ds_name,\n",
    "        split=\"train\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "    y_train = train_df[args.label_col]\n",
    "\n",
    "    test_df = load_dataset(\n",
    "        ds_name,\n",
    "        split=\"test\",  # download_mode=\"force_redownload\"\n",
    "    ).to_pandas()\n",
    "\n",
    "    # Models\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.text_model_base, model_max_length=512\n",
    "    )\n",
    "    if args.model_type in all_text_versions:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=args.my_text_model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert all columns to a single string\n",
    "        if args.model_type in [\"all_as_text\", \"all_text\"]:\n",
    "            def cols_to_str_fn(array): return \" | \".join(\n",
    "                [\n",
    "                    f\"{col}: {val}\"\n",
    "                    for col, val in zip(\n",
    "                        args.categorical_cols + args.numerical_cols + args.text_cols,\n",
    "                        array,\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # # Reorder based on the new index order in di\n",
    "            # cols_to_str_fn = lambda array: \" | \".join(\n",
    "            #     [\n",
    "            #         f\"{col}: {val}\"\n",
    "            #         for _, col, val in sorted(\n",
    "            #             zip(args.new_idx_order, args.tab_cols + args.text_cols, array)\n",
    "            #         )\n",
    "            #     ]\n",
    "            # )\n",
    "            raise NotImplementedError(\n",
    "                \"Shouldn't need much as the column ordering is in dataset info,\\\n",
    "                just need to update the cols_to_str_fn\"\n",
    "            )\n",
    "\n",
    "        model = AllAsTextModel(\n",
    "            text_pipeline=text_pipeline,\n",
    "            cols_to_str_fn=cols_to_str_fn,\n",
    "        )\n",
    "    else:\n",
    "        text_pipeline = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=args.my_text_model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=\"cuda:0\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            top_k=None,\n",
    "        )\n",
    "        # Define how to convert the text columns to a single string\n",
    "        if len(args.text_cols) == 1:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return array[0]\n",
    "\n",
    "        else:\n",
    "\n",
    "            def cols_to_str_fn(array):\n",
    "                return \" | \".join(\n",
    "                    [f\"{col}: {val}\" for col, val in zip(\n",
    "                        args.text_cols, array)]\n",
    "                )\n",
    "\n",
    "        # LightGBM requires explicitly marking categorical features\n",
    "        train_df[args.categorical_cols] = train_df[args.categorical_cols].astype(\n",
    "            \"category\"\n",
    "        )\n",
    "        test_df[args.categorical_cols] = test_df[args.categorical_cols].astype(\n",
    "            \"category\"\n",
    "        )\n",
    "\n",
    "        tab_model = lgb.LGBMClassifier(random_state=42, max_depth=3)\n",
    "        tab_model.fit(train_df[args.categorical_cols +\n",
    "                      args.numerical_cols], y_train)\n",
    "\n",
    "        if args.model_type in [\"ensemble_25\", \"ensemble_50\", \"ensemble_75\"]:\n",
    "            text_weight = float(args.model_type.split(\"_\")[-1]) / 100\n",
    "            model = WeightedEnsemble(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                text_weight=text_weight,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "            )\n",
    "        elif args.model_type == \"stack\":\n",
    "            \"\"\"\n",
    "            For the stack model, we make predictions on the validation set. These predictions\n",
    "            are then used as features for the stack model (another LightGBM model) along with\n",
    "            the other tabular features. In doing so the stack model learns, depending on the\n",
    "            tabular features, when to trust the tabular model and when to trust the text model.\n",
    "            \"\"\"\n",
    "            val_df = load_dataset(\n",
    "                ds_name,\n",
    "                split=\"validation\",  # download_mode=\"force_redownload\"\n",
    "            ).to_pandas()\n",
    "            val_df[args.categorical_cols] = val_df[args.categorical_cols].astype(\n",
    "                \"category\"\n",
    "            )\n",
    "            y_val = val_df[args.label_col]\n",
    "            val_text = list(map(cols_to_str_fn, val_df[args.text_cols].values))\n",
    "\n",
    "            # Training set is the preditions from the tabular and text models on the validation set\n",
    "            # plus the tabular features from the validation set\n",
    "            text_val_preds = text_pipeline(val_text)\n",
    "            text_val_preds = np.array(\n",
    "                [format_text_pred(pred) for pred in text_val_preds]\n",
    "            )\n",
    "            # text_val_preds = np.array(\n",
    "            #     [[lab[\"score\"] for lab in pred] for pred in text_val_preds]\n",
    "            # )\n",
    "\n",
    "            # add text and tabular predictions to the val_df\n",
    "            stack_val_df = val_df[args.categorical_cols + args.numerical_cols]\n",
    "            tab_val_preds = tab_model.predict_proba(\n",
    "                val_df[args.categorical_cols + args.numerical_cols]\n",
    "            )\n",
    "            stack_val_df[f\"text_pred\"] = text_val_preds[:, 1]\n",
    "            stack_val_df[f\"tab_pred\"] = tab_val_preds[:, 1]\n",
    "\n",
    "            stack_model = lgb.LGBMClassifier(\n",
    "                random_state=42, max_depth=2, learning_rate=0.01\n",
    "            )\n",
    "            stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "            model = StackModel(\n",
    "                tab_model=tab_model,\n",
    "                text_pipeline=text_pipeline,\n",
    "                stack_model=stack_model,\n",
    "                cols_to_str_fn=cols_to_str_fn,\n",
    "                all_labels=False,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid model type of {args.model_type}\")\n",
    "\n",
    "    np.random.seed(1)\n",
    "    x = test_df[args.categorical_cols +\n",
    "                args.numerical_cols + args.text_cols].values\n",
    "    preds = model.predict(x)\n",
    "    labels = test_df[args.label_col].values\n",
    "    return preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vet_10c_all_text\n",
      "Updating with:\n",
      "{'config': 'vet_10c_all_text', 'my_text_model': 'james-burton/vet_10c', 'ds_name': 'james-burton/vet_month_1c_all_text', 'text_model_base': 'bert-base-uncased', 'model_type': 'all_text', 'ord_ds_name': 'james-burton/vet_month_1c_ordinal', 'text_cols': ['breed', 'region', 'record']}\n",
      "\n",
      "\n",
      "{'categorical_cols': ['gender', 'neutered', 'species', 'insured'], 'numerical_cols_long': ['age_at_consult', 'Diseases of the ear or mastoid process', 'Mental, behavioural or neurodevelopmental disorders', 'Diseases of the blood or blood-forming organs', 'Diseases of the circulatory system', 'Dental', 'Developmental anomalies', 'Diseases of the digestive system', 'Endocrine, nutritional or metabolic diseases', 'Diseases of the Immune system', 'Certain infectious or parasitic diseases', 'Diseases of the skin', 'Diseases of the musculoskeletal system or connective tissue', 'Neoplasms', 'Diseases of the nervous system', 'Diseases of the visual system', 'Certain conditions originating in the perinatal period', 'Pregnancy, childbirth or the puerperium', 'Diseases of the respiratory system', 'Injury, poisoning or certain other consequences of external causes', 'Diseases of the genitourinary system'], 'numerical_cols': ['age_at_consult', 'Ear_or_Mastoid', 'Mental_Behavioral_or_Neuro', 'Blood_or_Blood-forming', 'Circulatory', 'Dental', 'Developmental', 'Digestive', 'Endocrine_Nutritional_or_Metabolic', 'Immune', 'Infectious_or_Parasitic', 'Skin', 'Musculoskeletal_or_Connective_Tissue', 'Neoplasms', 'Nervous', 'Visual', 'Perinatal', 'Pregnancy_Childbirth_or_Puerperium', 'Respiratory', 'Injury_Poisoning_or_External_Causes', 'Genitourinary'], 'text_cols': ['breed', 'region', 'record'], 'label_col': 'labels', 'label_names': ['alive', 'dead'], 'config': 'vet_10c_all_text', 'my_text_model': 'james-burton/vet_10c', 'ds_name': 'james-burton/vet_month_1c_all_text', 'text_model_base': 'bert-base-uncased', 'model_type': 'all_text', 'ord_ds_name': 'james-burton/vet_month_1c_ordinal'}\n",
      "\n",
      "{'accuracy': 0.6423992673992674, 'precision': 0.6049079754601226, 'recall': 0.8780053428317008, 'f1': 0.7163094805666546}\n"
     ]
    }
   ],
   "source": [
    "for config_type in [\n",
    "    # \"vet_59c_stack\",\n",
    "    # \"vet_59c_ensemble_25\",\n",
    "    # \"vet_59c_ensemble_50\",\n",
    "    # \"vet_59c_ensemble_75\",\n",
    "    # \"vet_50c_all_text\",\n",
    "    # \"vet_19c_stack\",\n",
    "    # \"vet_19c_ensemble_25\",\n",
    "    # \"vet_19c_ensemble_50\",\n",
    "    # \"vet_19c_ensemble_75\",\n",
    "    \"vet_10c_all_text\",\n",
    "]:\n",
    "    print(config_type)\n",
    "    preds, labels = get_prediction(config_type)\n",
    "    try:\n",
    "        results = compute_metrics((preds, labels), argmax=True)\n",
    "    except:\n",
    "        results = compute_metrics((preds, labels), argmax=False)\n",
    "    print(results)\n",
    "# preds, labels = get_prediction(\"vet_59b_ensemble_50\")\n",
    "# results = compute_metrics((preds, labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating with:\n",
      "{'config': 'vet_19c_stack', 'my_text_model': 'james-burton/vet_19c', 'ds_name': 'james-burton/vet_month_1c_ordinal', 'text_model_base': 'bert-base-uncased', 'model_type': 'stack', 'ord_ds_name': 'james-burton/vet_month_1c_ordinal', 'text_cols': ['breed', 'region', 'record']}\n",
      "\n",
      "\n",
      "{'categorical_cols': ['gender', 'neutered', 'species', 'insured'], 'numerical_cols_long': ['age_at_consult', 'Diseases of the ear or mastoid process', 'Mental, behavioural or neurodevelopmental disorders', 'Diseases of the blood or blood-forming organs', 'Diseases of the circulatory system', 'Dental', 'Developmental anomalies', 'Diseases of the digestive system', 'Endocrine, nutritional or metabolic diseases', 'Diseases of the Immune system', 'Certain infectious or parasitic diseases', 'Diseases of the skin', 'Diseases of the musculoskeletal system or connective tissue', 'Neoplasms', 'Diseases of the nervous system', 'Diseases of the visual system', 'Certain conditions originating in the perinatal period', 'Pregnancy, childbirth or the puerperium', 'Diseases of the respiratory system', 'Injury, poisoning or certain other consequences of external causes', 'Diseases of the genitourinary system'], 'numerical_cols': ['age_at_consult', 'Ear_or_Mastoid', 'Mental_Behavioral_or_Neuro', 'Blood_or_Blood-forming', 'Circulatory', 'Dental', 'Developmental', 'Digestive', 'Endocrine_Nutritional_or_Metabolic', 'Immune', 'Infectious_or_Parasitic', 'Skin', 'Musculoskeletal_or_Connective_Tissue', 'Neoplasms', 'Nervous', 'Visual', 'Perinatal', 'Pregnancy_Childbirth_or_Puerperium', 'Respiratory', 'Injury_Poisoning_or_External_Causes', 'Genitourinary'], 'text_cols': ['breed', 'region', 'record'], 'label_col': 'labels', 'label_names': ['alive', 'dead'], 'config': 'vet_19c_stack', 'my_text_model': 'james-burton/vet_19c', 'ds_name': 'james-burton/vet_month_1c_ordinal', 'text_model_base': 'bert-base-uncased', 'model_type': 'stack', 'ord_ds_name': 'james-burton/vet_month_1c_ordinal'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 727/727 [00:00<00:00, 1.10MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:10<00:00, 43.8MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3676, number of negative: 3530\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000374 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 525\n",
      "[LightGBM] [Info] Number of data points in the train set: 7206, number of used features: 24\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.510130 -> initscore=0.040527\n",
      "[LightGBM] [Info] Start training from score 0.040527\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "config_type = \"vet_19c_stack\"\n",
    "\n",
    "di = ConfigLoader(\n",
    "    config_type, \"../configs/shap_configs.yaml\", \"../configs/dataset_default.yaml\"\n",
    ")\n",
    "# Data\n",
    "train_df = load_dataset(\n",
    "    di.ds_name,\n",
    "    split=\"train\",  # download_mode=\"force_redownload\"\n",
    ").to_pandas()\n",
    "y_train = train_df[di.label_col]\n",
    "\n",
    "test_df = load_dataset(\n",
    "    di.ds_name,\n",
    "    split=\"test\",  # download_mode=\"force_redownload\"\n",
    ").to_pandas()\n",
    "# test_df = test_df.sample(test_set_size, random_state=55)\n",
    "\n",
    "# Models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    di.text_model_base, model_max_length=512)\n",
    "text_pipeline = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=di.my_text_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=\"cuda:0\",\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    top_k=None,\n",
    ")\n",
    "# Define how to convert the text columns to a single string\n",
    "if len(di.text_cols) == 1:\n",
    "\n",
    "    def cols_to_str_fn(array):\n",
    "        return array[0]\n",
    "\n",
    "else:\n",
    "\n",
    "    def cols_to_str_fn(array):\n",
    "        return \" | \".join([f\"{col}: {val}\" for col, val in zip(di.text_cols, array)])\n",
    "\n",
    "\n",
    "# LightGBM requires explicitly marking categorical features\n",
    "train_df[di.categorical_cols] = train_df[di.categorical_cols].astype(\n",
    "    \"category\")\n",
    "test_df[di.categorical_cols] = test_df[di.categorical_cols].astype(\"category\")\n",
    "\n",
    "tab_model = lgb.LGBMClassifier(random_state=42, max_depth=3)\n",
    "tab_model.fit(train_df[di.categorical_cols + di.numerical_cols], y_train)\n",
    "\n",
    "\"\"\"\n",
    "For the stack model, we make predictions on the validation set. These\n",
    "predictions are then used as features for the stack model (another LightGBM\n",
    "model) along with the other tabular features. In doing so the stack model\n",
    "learns, depending on the tabular features, when to trust the tabular model\n",
    "and when to trust the text model.\n",
    "\"\"\"\n",
    "val_df = load_dataset(\n",
    "    di.ds_name,\n",
    "    split=\"validation\",  # download_mode=\"force_redownload\"\n",
    ").to_pandas()\n",
    "val_df[di.categorical_cols] = val_df[di.categorical_cols].astype(\"category\")\n",
    "y_val = val_df[di.label_col]\n",
    "val_text = list(map(cols_to_str_fn, val_df[di.text_cols].values))\n",
    "\n",
    "# Training set is the preditions from the tabular and text models on the\n",
    "# validation set plus the tabular features from the validation set\n",
    "text_val_preds = text_pipeline(val_text)\n",
    "text_val_preds = np.array([format_text_pred(pred) for pred in text_val_preds])\n",
    "\n",
    "# add text and tabular predictions to the val_df\n",
    "stack_val_df = val_df[di.categorical_cols + di.numerical_cols]\n",
    "# stack_val_df = val_df[di.numerical_cols].copy()\n",
    "tab_val_preds = tab_model.predict_proba(\n",
    "    val_df[di.categorical_cols + di.numerical_cols])\n",
    "stack_val_df[f\"text_pred\"] = text_val_preds[:, 1]\n",
    "stack_val_df[f\"tab_pred\"] = tab_val_preds[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "test_df[di.categorical_cols] = test_df[di.categorical_cols].astype(\"category\")\n",
    "test_text = list(map(cols_to_str_fn, test_df[di.text_cols].values))\n",
    "\n",
    "text_test_preds = text_pipeline(test_text)\n",
    "text_test_preds = np.array([format_text_pred(pred)\n",
    "                           for pred in text_test_preds])\n",
    "\n",
    "# add text and tabular predictions to the test_df\n",
    "stack_test_df = test_df[di.categorical_cols + di.numerical_cols]\n",
    "# stack_test_df = test_df[di.numerical_cols].copy()\n",
    "tab_test_preds = tab_model.predict_proba(\n",
    "    test_df[di.categorical_cols + di.numerical_cols]\n",
    ")\n",
    "stack_test_df[f\"text_pred\"] = text_test_preds[:, 1]\n",
    "stack_test_df[f\"tab_pred\"] = tab_test_preds[:, 1]\n",
    "y_test = test_df[di.label_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 649, number of negative: 623\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 945\n",
      "[LightGBM] [Info] Number of data points in the train set: 1272, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.510220 -> initscore=0.040886\n",
      "[LightGBM] [Info] Start training from score 0.040886\n",
      "{'accuracy': 0.7099056603773585, 'precision': 0.6983002832861189, 'recall': 0.7596302003081664, 'f1': 0.7276752767527676}\n",
      "{'accuracy': 0.6762820512820513, 'precision': 0.6674718196457327, 'recall': 0.7382012466607302, 'f1': 0.7010570824524314}\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "stack_model = lgb.LGBMClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=2,\n",
    "    learning_rate=0.01,\n",
    ")\n",
    "stack_model.fit(stack_val_df, y_val)\n",
    "\n",
    "preds = stack_model.predict(stack_val_df)\n",
    "labels = y_val.values\n",
    "print(compute_metrics((preds, labels), argmax=False))\n",
    "\n",
    "# test set\n",
    "preds = stack_model.predict(stack_test_df)\n",
    "labels = y_test.values\n",
    "print(compute_metrics((preds, labels), argmax=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sean simple tree combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5408805031446541, 'precision': 0.5276595744680851, 'recall': 0.9553158705701078, 'f1': 0.6798245614035087}\n",
      "{'accuracy': 0.5018315018315018, 'precision': 0.5086934923000497, 'recall': 0.9118432769367765, 'f1': 0.653061224489796}\n",
      "{'accuracy': 0.6515567765567766, 'precision': 0.6348733233979136, 'recall': 0.7586821015138023, 'f1': 0.6912778904665314}\n",
      "{'accuracy': 0.6364468864468864, 'precision': 0.6362883181441591, 'recall': 0.6838824577025824, 'f1': 0.6592274678111587}\n"
     ]
    }
   ],
   "source": [
    "tree_model = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "tree_val_df = np.vstack(\n",
    "    [text_val_preds[:, 1], tab_val_preds[:, 1]]).reshape(-1, 2)\n",
    "tree_model.fit(tree_val_df, y_val)\n",
    "\n",
    "preds = tree_model.predict(tree_val_df)\n",
    "labels = y_val.values\n",
    "print(compute_metrics((preds, labels), argmax=False))\n",
    "\n",
    "tree_test_df = np.vstack(\n",
    "    [text_test_preds[:, 1], tab_test_preds[:, 1]]).reshape(-1, 2)\n",
    "preds = tree_model.predict(tree_test_df)\n",
    "labels = y_test.values\n",
    "print(compute_metrics((preds, labels), argmax=False))\n",
    "\n",
    "# Text only\n",
    "print(compute_metrics((text_test_preds, y_test.values), argmax=True))\n",
    "\n",
    "# Tabular only\n",
    "print(compute_metrics((tab_test_preds, y_test.values), argmax=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training set accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6770746600055509, 'precision': 0.669685534591195, 'recall': 0.7241566920565833, 'f1': 0.6958567507515357}\n",
      "{'accuracy': 0.7212045517624202, 'precision': 0.6906886296042095, 'recall': 0.8212731229597389, 'f1': 0.7503417422641978}\n"
     ]
    }
   ],
   "source": [
    "tab_train_preds = tab_model.predict(\n",
    "    train_df[di.categorical_cols + di.numerical_cols])\n",
    "print(compute_metrics((tab_train_preds, y_train.values), argmax=False))\n",
    "text_train_preds = text_pipeline(\n",
    "    list(map(cols_to_str_fn, train_df[di.text_cols].values))\n",
    ")\n",
    "text_train_preds = np.array([format_text_pred(pred)\n",
    "                            for pred in text_train_preds])\n",
    "print(compute_metrics((text_train_preds, y_train.values), argmax=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
